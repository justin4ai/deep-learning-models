{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style = \"background-color : skyblue\"> Generative Adversarial Networks Template </span> </h1>\n",
    "12 Fabruary, 2021 - <strong>Junyeong Ahn</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.5)\n",
    "sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test displaying of mnist data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x, y = mnist.load_data()\n",
    "\n",
    "cv2.imshow(\"MNIST_TEST\", x[0][0])\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define input image dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1 # grayscale\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define generator network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given input of noise (latent) vector, the Generaotr produces an image.\n",
    "def build_generator():\n",
    "    \n",
    "    noise_shape = (100,) # 1D array of size 100 (latent vector / noise)\n",
    "    \n",
    "    # Here we are only using Dense layers. But network can be complicated\n",
    "    # based on the application. For example, you can use VGG for super res. GAN.\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_shape = noise_shape))\n",
    "    model.add(LeakyReLU(alpha = 0.2)) # 0.2 is from published paper\n",
    "    model.add(BatchNormalization(momentum = 0.8)) # How fast it trains\n",
    "    \n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    model.add(BatchNormalization(momentum = 0.8))\n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha = 0.2)) \n",
    "    model.add(BatchNormalization(momentum = 0.8))\n",
    "    \n",
    "    model.add(Dense(np.prod(img_shape), activation = \"tanh\"))\n",
    "    model.add(Reshape(img_shape))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    noise = Input(shape = noise_shape)\n",
    "    img = model(noise) # Generated image (fake)\n",
    "    \n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given an input image, the Discriminator outputs the likelihood of the image being real.**   \n",
    "- **Binary classification - true or false (*we're calling it validity)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define discriminator network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten(input_shape = img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    img = Input(shape = img_shape)\n",
    "    validity = model(img) # the Discriminator's guess of input being real or not.\n",
    "    \n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have constructed our two models it's time to pit them against each other.**   \n",
    "   \n",
    "**We do this by defining a training fuction, loading the data set, re-scaling our training images and setting the ground thrughts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define train function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size = 128, save_interval = 500):\n",
    "    \n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "    \n",
    "    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    \n",
    "    # Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n",
    "    X_train = np.expand_dims(X_train, axis = 3)\n",
    "    half_batch = int(batch_size / 3)\n",
    "    \n",
    "\n",
    "# We then loop through  a number of epochs to train our Discriminator by first selecting\n",
    "# a random batch of images from our true dataset, generating a set of images from our Generator,\n",
    "# feeding both set of images into our Discriminator, and finally setting the\n",
    "# loss parameters for both the real and fake images, as well as the combined loss.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # ----------------------\n",
    "        #  Train Discriminator\n",
    "        # ----------------------\n",
    "        \n",
    "        # Select a random half batch of real images\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        imgs = X_train[idx]\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (half_batch, 100)) # half_batch x 100 matrix based on Gaussian Distribution\n",
    "        \n",
    "        # Generate a half batch of fake images\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        \n",
    "        # Train the discriminator on real and fake images, separately\n",
    "        # Research showed that separate training is more effective.\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, np.ones( (half_batch, 1))) # 1 for real images\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros( (half_batch, 1))) # 0 for fake images\n",
    "        \n",
    "        # Take average loss from real and fake images.\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        \n",
    "# And within the same loop we train our Generator, by setting the input noise and\n",
    "# ultimately training the Generator to have the Discriminator label its samples as valid\n",
    "# by specifying the gradient loss.\n",
    "        \n",
    "        # ----------------------\n",
    "        #  Train Generator\n",
    "        # ----------------------\n",
    "# Create noise vectors as input for generator.\n",
    "# Create as many noise vectors as defined by the batch size.\n",
    "# Based on normal distribution. Output will be of size (batch size, 100)\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        \n",
    "        # The generator wants the discriminator to label the generated samples as valid ones\n",
    "        # This is where the generator is trying to trick discriminator into believing\n",
    "        # the generated image is true (hence value of 1 for y)\n",
    "        valid_y = np.array([1] * batch_size) # Creates an array of all ones of size = batch size\n",
    "        \n",
    "        # Generator is part of combined where it got directly linked with the discriminator\n",
    "        # Train the generator with noise as x and 1 as y.\n",
    "        # Again, 1 as the output as it is adversarial and if generaotr did a great job\n",
    "        # of fooling the discriminator then the output would be 1 (true)\n",
    "        g_loss = combined.train_on_batch(noise, valid_y)\n",
    "        \n",
    "\n",
    "# Additionally, in order for us to keep track of our training process, we print the\n",
    "# progress and save the sameple image output depending on the epoch interval specified.\n",
    "\n",
    "        # Plot the progress\n",
    "        #print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], ))\n",
    "        \n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When the specific sample_interval is hit, we call the sample_image function,\n",
    "which looks as follows.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define image-saver that shows fake image qualities over time</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function saves our images for us to view\n",
    "def save_imgs(epoch):\n",
    "    \n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    \n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap = \"gray\")\n",
    "            axs[i, j].axis(\"off\")\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images\\\\mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> <span style = \"background-color : yellow\"> Main Part</span> <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.0002, 0.5) # Learning rate and momentum\n",
    "\n",
    "# Build and compile the discriminator first.\n",
    "# Generator will be trained as part of the compined model, later.\n",
    "# Pick the loss function and the type of metric to keep track.\n",
    "# Binary cross entropy as we are doing prediction and it is a better loss function compared to MSE or others.\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss = \"binary_crossentropy\",\n",
    "                     optimizer = optimizer,\n",
    "                     metrics = [\"accuracy\"])\n",
    "\n",
    "# Build and compile our Discriminator, pick the loss function\n",
    "# Since we are only generating (faking) images, let us not track any metrics.\n",
    "generator = build_generator()\n",
    "generator.compile(loss = \"binary_crossentropy\", optimizer = optimizer)\n",
    "\n",
    "# This builds the Generator and defines the input noise.\n",
    "# In a GAN the Generator network takes noise z as an input to produce its images.\n",
    "z = Input(shape = (100,)) # Our random input to the generator\n",
    "img = generator(z)\n",
    "\n",
    "# This ensures that when we combine our networks *we only train the Generator*.\n",
    "# While Generator training we do not want Discriminator weights to be adjusted.\n",
    "# This dosen't affect the above Discriminator training.\n",
    "discriminator.trainable = False\n",
    "\n",
    "# This specifies that our Discriminator will take the images generated by our Generator\n",
    "# and true dataset and set its output to a parameter called valid, which whill indicate\n",
    "# whether the input is real or not.\n",
    "valid = discriminator(img) # Validity check on the generated image\n",
    "\n",
    "# Here we combined the models and also set our loss function and optimizer.\n",
    "# Again, we are only training the Generator here.\n",
    "# The ultimate goal here is for the Generator to fool the Discriminator.\n",
    "# The combined model (stacked Gen. and Discr.) takes noise as input => generates images => determines validity\n",
    "combined = Model(z, valid) # Input(z) : image / Output(valid) : validity\n",
    "combined.compile(loss = \"binary_crossentropy\", optimizer = optimizer)\n",
    "\n",
    "train(epochs = 2000, batch_size = 32, save_interval = 20)\n",
    "\n",
    "# Save model for future use to generate fake images\n",
    "# Not tested yet..l make sure right model is being saved..\n",
    "# Compare with GAN4\n",
    "\n",
    "generator.save(\"generator_model_test.h5\") # Test the model on GAN4_predict...\n",
    "\n",
    "# Epochs dictate the number of backward and forward propagations, the batch_size\n",
    "# indicates the number of training smaples per backward/forward propagation, and\n",
    "# sample_interval specifies after how many epochs we call our sample_image function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
