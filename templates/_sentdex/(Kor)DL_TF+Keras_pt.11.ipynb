{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentdex - 딥러닝 Pt.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 목표 : 드디어 예측!</h3>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.6)\n",
    "sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 69188 validation: 3062\n",
      "Dont buys: 34594, buys: 34594\n",
      "VALIDATION Dont buys: 1531, buys: 1531\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization #DuDNNLSTM\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "# 60분을 통해 3분 (미래) 예측\n",
    "\n",
    "SEQ_LEN = 60\n",
    "FUTURE_PERIOD_PREDICT = 3\n",
    "RATIO_TO_PREDICT = \"LTC-USD\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    \n",
    "    # \"future\" 열은 이제 필요 없음\n",
    "    df = df.drop(\"future\", axis = 1)\n",
    "    \n",
    "    # scaling\n",
    "    for col in df.columns:\n",
    "        if col != \"target\":\n",
    "            \n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace = True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "    \n",
    "    # 혹시 모르니까..\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    \n",
    "    sequential_data = []\n",
    "    prev_mins = deque(maxlen = SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:  # df.values: df의 각 행\n",
    "        prev_mins.append([n for n in i[:-1]])  # why i[:-1]? : target class를 미포함시키기 위해서\n",
    "        \n",
    "        if len(prev_mins) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_mins), i[-1]])  # 지난 60일 ( (i)일 기준 ) + _일의 타겟값(i)\n",
    "            \n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    # 밸런싱합시다!\n",
    "    # 사야될 날과 안되는 날을 구분\n",
    "    buys, sells = [], []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "            \n",
    "    # 섞어섞어\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    \n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    # 최소 사이즈에 맞춰서 둘 다 잘라버림 - 밸런싱\n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    # 연쇄\n",
    "    sequential_data = buys + sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    # X와 y로 스플릿\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X), y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "ratios = [\"BTC-USD\", \"LTC-USD\", \"ETH-USD\", \"BCH-USD\"]\n",
    "for ratio in ratios:\n",
    "    dataset = f\"crypto_data/{ratio}.csv\"\n",
    "    \n",
    "    df = pd.read_csv(dataset,\n",
    "                     names = [\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\n",
    "    df.rename(columns = {\"close\" : f\"{ratio}_close\", \"volume\" : f\"{ratio}_volume\"}, inplace = True)\n",
    "    \n",
    "    df.set_index(\"time\", inplace = True)\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
    "    \n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "        \n",
    "# \"future\" 값은 사실 \"close\"가 3일치 밀린 (shifted) 값이다!\n",
    "main_df[\"future\"] = main_df[ f\"{RATIO_TO_PREDICT}_close\" ].shift(-FUTURE_PERIOD_PREDICT)        \n",
    "\n",
    "# 분류합시다 (ML은 아니지만)\n",
    "main_df['target'] = list(map(classify, main_df[ f\"{RATIO_TO_PREDICT}_close\"], main_df[\"future\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "times = sorted(main_df.index.values)\n",
    "last_5pct = times[-int(0.05 * len(times))] # threshold\n",
    "\n",
    "validation_main_df = main_df[ (main_df.index >= last_5pct)] # validation (5%)\n",
    "main_df = main_df[ (main_df.index < last_5pct)] # the rest\n",
    "\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서보드 이용법\n",
    "\n",
    "1. **모델 피팅 코드 (아래) 돌리기**\n",
    "2. **돌리는 동안 cmd 열기**\n",
    "3. **\" tensorboard --logdir=\"DL_Recap/sentdex/logs/\" (디렉토리 위치는 컴퓨터 환경에 따라 바뀔 수 있음) 커맨드 입력**\n",
    "4. **cmd 켜놓은 상태로 http://localhost:6006에 접속**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**로그를 볼 수 없는 오류는?**    https://github.com/tensorflow/tensorboard/issues/3117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #CuDNNLSTM 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69188 samples, validate on 3062 samples\n",
      "Epoch 1/10\n",
      "69188/69188 [==============================] - 18s 254us/sample - loss: 0.7231 - acc: 0.5061 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "69188/69188 [==============================] - 16s 237us/sample - loss: 0.6908 - acc: 0.5306 - val_loss: 0.6901 - val_acc: 0.5284\n",
      "Epoch 3/10\n",
      "69188/69188 [==============================] - 16s 235us/sample - loss: 0.6917 - acc: 0.5233 - val_loss: 0.6962 - val_acc: 0.5036\n",
      "Epoch 4/10\n",
      "69188/69188 [==============================] - 16s 234us/sample - loss: 0.6875 - acc: 0.5457 - val_loss: 0.6842 - val_acc: 0.5572\n",
      "Epoch 5/10\n",
      "69188/69188 [==============================] - 16s 235us/sample - loss: 0.6856 - acc: 0.5545 - val_loss: 0.7147 - val_acc: 0.5356\n",
      "Epoch 6/10\n",
      "69188/69188 [==============================] - 16s 235us/sample - loss: 0.6842 - acc: 0.5582 - val_loss: 0.6794 - val_acc: 0.5617\n",
      "Epoch 7/10\n",
      "69188/69188 [==============================] - 17s 240us/sample - loss: 0.6844 - acc: 0.5588 - val_loss: 0.6799 - val_acc: 0.5656\n",
      "Epoch 8/10\n",
      "69188/69188 [==============================] - 16s 235us/sample - loss: 0.6814 - acc: 0.5645 - val_loss: 0.6810 - val_acc: 0.5679\n",
      "Epoch 9/10\n",
      "69188/69188 [==============================] - 16s 237us/sample - loss: 0.6814 - acc: 0.5640 - val_loss: 0.6789 - val_acc: 0.5640\n",
      "Epoch 10/10\n",
      "69188/69188 [==============================] - 16s 236us/sample - loss: 0.6795 - acc: 0.5675 - val_loss: 0.6781 - val_acc: 0.5686\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import CuDNNLSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add( CuDNNLSTM(128, input_shape = (train_x.shape[1:]), return_sequences = True) )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add( CuDNNLSTM(128, input_shape = (train_x.shape[1:]), return_sequences = True) )\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add( CuDNNLSTM(128, input_shape = (train_x.shape[1:])) ) # 다음에 오는게 Dense layer라서 return_sequences는 필요 없음\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation = \"softmax\"))  # 둘 중 하나! (일반 classification)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr = 0.001, decay = 1e-6)\n",
    "\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = opt,\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(log_dir = f\"logs\\\\{NAME}\")\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:3f}\" # 에폭에 대해서 에폭-validation 정확도를 저장할 유니크한 저장 경로\n",
    "checkpoint = ModelCheckpoint(\"models\\\\{}.model\".format(filepath, moniter = \"val_acc\",\n",
    "                                                     verbose = 1, save_best_only = True, mode = \"max\")) # 최고들만 저장\n",
    "\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (validation_x, validation_y),\n",
    "    callbacks = [tensorboard, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #LSTM 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hewas\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 69188 samples, validate on 3062 samples\n",
      "WARNING:tensorflow:From C:\\Users\\hewas\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "55488/69188 [=======================>......] - ETA: 46s - loss: 0.7249 - acc: 0.5107"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bcaad13e8654>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     callbacks = [tensorboard, checkpoint])\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( LSTM(128, input_shape = (train_x.shape[1:]),\n",
    "                activation = \"relu\", return_sequences = True) )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add( LSTM(128, input_shape = (train_x.shape[1:]),\n",
    "                activation = \"relu\", return_sequences = True) )\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add( LSTM(128, input_shape = (train_x.shape[1:]),\n",
    "                activation = \"relu\") ) # 다음에 오는게 Dense layer라서 return_sequences는 필요 없음\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation = \"softmax\"))  # 둘 중 하나! (일반 classification)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr = 0.001, decay = 1e-6)\n",
    "\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = opt,\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(log_dir = f\"logs\\\\{NAME}\")\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:3f}\" # 에폭에 대해서 에폭-validation 정확도를 저장할 유니크한 저장 경로\n",
    "checkpoint = ModelCheckpoint(\"models\\\\{}.model\".format(filepath, moniter = \"val_acc\",\n",
    "                                                     verbose = 1, save_best_only = True, mode = \"max\")) # 최고들만 저장\n",
    "\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (validation_x, validation_y),\n",
    "    callbacks = [tensorboard, checkpoint])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
